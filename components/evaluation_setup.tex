\section{Evaluation Setup}
\label{ss:sec:evaluation_setup}

\smallskip
\noindent
\textbf{Research Questions:} We answer the following research questions:

\smallskip
\textbf{RQ1 (State Initialization)}
How well does \spoilsport find meaningful starting states on rug pull smart contracts?

\smallskip
\textbf{RQ2 (Correlation with Known Rug Pull Types:)}
In searching for goal violations, does \spoilsport also uncover the root causes of rug pulls?

\smallskip
\textbf{RQ3 (New Information of Rug Pull Mechanisms:)}
Do the found goal violations effectively disclose new information about rug pull mechanisms in their contracts?

\smallskip
\textbf{RQ4 (Ablation of Control Flow Guidance)}
How much does the control flow guidance contribute to finding state initialization and goal violations?


\smallskip
\textbf{RQ1} investigates the efficacy of finding sequences of transactions to transform the smart contract states into ones which resemble characteristics of expected usage.
\textbf{RQ2} and \textbf{RQ3} inspect the relationship between found goal violations and trap doors in rug pulls.
Our evaluation does not aim to detect rug pulls in the wild, but rather enrich the ground truth of contracts which are %known to be 
malicious.
In \textbf{RQ2}, we present goal violation results and investigate whether these violations correspond to the dataset's labelled rug pull types.
In \textbf{RQ3}, we investigate the goal violations in more detail.
While our dataset gives us a high-level classification of the rug pull trap door, we wish to understand whether there is useful additional detail about rug pull mechanics which we may retrieve from goal violations.
In \textbf{RQ4}, we evaluate the efficacy of using the function-level control flow dependencies to guide seed mutation by removing this guidance.

\smallskip
\noindent 
\noindent \textbf{Heuristic-based detection of key variables:}
    We implement a heuristic-driven detection methodology for the key variables of the \texttt{balances} mapping variable (tracking user token holdings) and \texttt{totalSupply} variable (the sum of all issued tokens), which may take arbitrary names.
    This is loosely adapted from prior work~\cite{fairchecker}.
    We make use of functions mandated by the ERC-20 specification~\cite{ERC20TokenStandard}.
    For \texttt{balances}, we retrieve the type-matched variable which is read by \texttt{balanceOf()} and written by \texttt{transfer()}.
    If no candidate exists, we additionally check for a variable named \texttt{balanceOf} as that would result in an implicit \texttt{balanceOf} function.
    For \texttt{totalSupply}, we retrieve the type-matched variable which is read by \texttt{totalSupply()} and written by \texttt{transfer()}.
    If no candidate exists, we additionally check for a variable named \texttt{totalSupply} as that would result in an implicit \texttt{totalSupply} function.



\smallskip
\noindent
\textbf{Setup for evaluation:}
We use a dataset from prior work on rug pulls~\cite{SoKRugPull2025}, labelling 2,391 known rug pull contracts under their proposed root cause taxonomy.
\autoref{ss:fig:dataset-filtration} illustrates our process of filtering to the 1,376 (57.5\%) contracts used for our evaluation.
Of these, 2,279 have Solidity source code available from Etherscan~\cite{etherscan}.
As we only define roles and goals for ERC-20 smart contracts, we filter these to ERC-20 contracts only (2,002 contracts).
To generalize our specification of roles/goals across contracts, we use heuristic-based detection of required key variables, leaving us with 1,680 contracts.
Additionally, our fuzzer does not currently support contracts which perform function calls to external dependencies.
This leaves us with a final 1,376 contracts for our evaluation.
We allocate a time budget of 30 seconds per contract.

\input{diagrams/diagram_rp_dataset_filtration.tex}

The dataset labels are high-level, specifying the kind of root cause of the rug pull as well classifying its type.
This does not allow for reproduction of the exact mechanism of the rug pull (i.e., it does not identify which functions or transactions were used to actuate the fraud).
As in the full dataset, the trap door labels in this sample are heavily skewed towards \textit{Transaction Limitation} violations (97.7\% of all entries).
Of these, 95.3\% are due to \textit{Sale-Restrict} which only allows privileged users to transfer their tokens, 3.1\% are due to \textit{Freeze Account} which prevent victim wallet addresses from performing transactions and
1.4\% are due to \textit{Transfer Block} which globally disables transfers.

\smallskip
\noindent 
\textbf{Optimizations:} To speed up the construction of transaction sequences, we implement the following optimizations:
\begin{enumerate}
    \item \textit{Initial corpuses optimizations:}
        To prioritize values at sign boundaries, we used initial corpuses of $\{-1, 0, 1\}$ for unsigned integer parameters and $\{0, 1\}$ for signed integer parameters.
        Once exhausted, random values based on bitwidth are generated.
        This is to prevent overshooting when conditionals bound input values.
        We also constrained address parameters to select from the available bounded address space only.
        Additionally, we retrieve any constant values from goal specification and inject those into the initial corpus.
    \item \textit{Bounded address space:}
        While addresses are normally 160-bit values, we constrain them to a small set with an adjustable parameter.
        During our static analysis stage, we retrieve hardcoded addresses from the Solidity source code as these are common in rug-pull contracts (e.g., used to encode privileged addresses).
    \item \textit{Revm testnet backend:} 
        We integrated the following EVM implementations as testnet backends for \spoilsport: Ethereum Tester~\cite{ethereum_tester}, Anvil~\cite{anvil} with JSON RPC and Revm~\cite{revm} with rust bindings to python.
        Of the three, the Revm integration was fastest at 6,962 transactions per second (TPS).
        Anvil and Ethereum Tester were significantly slower, with 91TPS and 44TPS respectively.
        We use the Revm integration for experiments as it is two orders of magnitude faster.
\end{enumerate}

\smallskip
\noindent
\textbf{\spoilsport configuration:}
We dynamically configure the number of users in our bounded address space to $\max{(hardcoded_{addresses} + 2, 3)}$.
This ensures that all hardcoded addresses are available as users while also including non-hardcoded users to represent typical end-users.
\todo{Did we talk about a solver? I think readers will be confused where the solver is playing a role for fuzzing.} For every function evaluated, 
we configure the solver to generate test input corresponding to the cartesian product of five values per parameter across all potential users as transactors.

During the \textit{State Initialization} step, we limit the maximum transaction sequence size to 20.
Additionally, we bias the initial contract deployer user to be a non-hardcoded address to increase realism.
During the \textit{Goal Violation Search} step, we limit the maximum transaction sequence size to two.
This is due to the majority of rug pull features identified in literature~\cite{trapdoor,SoKRugPull2025,pied-piper} requiring at most two transactions.


\smallskip
\noindent
\textbf{Frameworks and Platforms:}
We retrieve smart contract source code from Etherscan~\cite{EthMarketCap}.
During our \textit{Static Analysis} stage, we use Slither~\cite{Slither} to construct function summaries which contain data and control flow dependencies.
We use the Revm \cite{revm} Ethereum Virtual Machine (EVM) implementation as our execution backend.
The majority of the code is written in Python, with custom Rust bindings for Revm written in PyO3.
All evaluations were conducted on a 16-inch Macbook Pro (2021 model, M1 Max CPU).
